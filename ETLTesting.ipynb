{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established to:  Microsoft SQL Server 2019 (RTM-CU18) (KB5017593) - 15.0.4261.1 (X64) \n",
      "\tSep 12 2022 15:07:06 \n",
      "\tCopyright (C) 2019 Microsoft Corporation\n",
      "\tEnterprise Edition: Core-based Licensing (64-bit) on Windows Server 2016 Datacenter 10.0 <X64> (Build 14393: ) (Hypervisor)\n",
      "\n",
      "[('GCV_API', 'GCV_STG', 'NS-ManagementLevel-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_STG].NS-ManagementLevel-en\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-54cfce87e31b>:839: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:841: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:842: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:844: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\W+\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:839: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:841: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:842: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:844: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\W+\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:839: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:841: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:842: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:844: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\W+\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:839: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:841: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
      "<ipython-input-1-54cfce87e31b>:842: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  if df[date[1]].iteritems() != 'None':\n",
      "<ipython-input-1-54cfce87e31b>:844: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df[date[1]] = df[date[1]].str.replace(\"\\W+\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 Existing records\n",
      "NS-ManagementLevel-en has been updated with 12 records\n",
      "\n",
      "# of records in each table:  12 12\n",
      "[GCV_STG].NS-ManagementLevel-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "\n",
      "ID Check: Table does not contain a PK_ID\n",
      "\n",
      "\n",
      "Backup Duration: 0:00:00.231129\n",
      "\n",
      "\n",
      "Duration: 0:00:07.796450\n",
      "\n",
      "Total Execution Duration: 0:00:21.400224 \n",
      "-Import Completed-\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import warnings, sys\n",
    "import urllib\n",
    "import itertools\n",
    "from requests.exceptions import ConnectionError, HTTPError, Timeout, TooManyRedirects\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, event\n",
    "import json\n",
    "import time as ti\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import re\n",
    "import os\n",
    "import asyncio\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual, Layout\n",
    "import ipywidgets as widgets\n",
    "import ctypes\n",
    "import threading\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "global stage, onb, conn, df, recnum #, GCVList\n",
    "conn = False\n",
    "con = None\n",
    "\n",
    "#GCVList = ['GCV_PRD','GCV_STG','GCV_STG_ONB']\n",
    "try:\n",
    "    \n",
    "    logging.basicConfig(filename = 'CommonConfigLog.log',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                        datefmt='%H:%M:%S',\n",
    "                        level=logging.DEBUG)\n",
    "    logging.info(f'\\nLOG START: {datetime.now()}\\n')\n",
    "    \n",
    "    #def open_connection():\n",
    "    #    global server, database, driver, dformat, connection, con\n",
    "    #    with open(r'C:\\Users\\gmoye001\\config\\config.json', 'r') as fh:\n",
    "    #        config = json.load(fh)\n",
    "    #    server = config['server']\n",
    "    #    database = config['database']\n",
    "    #    driver = config['driver']\n",
    "    #    dformat = config['dformat']\n",
    "    #    connection = f'DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection=yes'\n",
    "    #    con = pyodbc.connect(connection)\n",
    "    #    return con\n",
    "    #\n",
    "    #def connection_test():\n",
    "    #    cur = con.cursor()\n",
    "    #    cur.execute(\"SELECT @@version\")\n",
    "    #    row = cur.fetchone()\n",
    "    #    cur.close()\n",
    "    #    con.commit()\n",
    "    #    logging.info(f\"\\nConnection established to: {row[0]}\\n\")\n",
    "    #    return print(\"Connection established to: \",row[0])\n",
    "    #    \n",
    "    #def close_connection():\n",
    "    #    con.close()\n",
    "    #    return\n",
    "    \n",
    "    def open_connection():\n",
    "        global server, database, driver, dformat, connection, con\n",
    "        with open(r\"config\\config.json\", 'r') as fh:\n",
    "            config = json.load(fh)\n",
    "        server = config['server']\n",
    "        database = config['database']\n",
    "        driver = config['driver']\n",
    "        dformat = config['dformat']\n",
    "        connection = f'DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection=yes'\n",
    "        con = pyodbc.connect(connection)\n",
    "        return con\n",
    "    \n",
    "    def connection_test():\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT @@version\")\n",
    "        row = cur.fetchone()\n",
    "        print(\"Connection established to: \",row[0])\n",
    "        cur.close()\n",
    "        con.commit()\n",
    "        logging.info(f\"\\nConnection established to: {row[0]}\\n\")\n",
    "        return \n",
    "        \n",
    "    def close_connection():\n",
    "        con.close()\n",
    "        conn = False\n",
    "        return\n",
    "    \n",
    "    def checktables(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_PRD' and table_name = '%s'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_PRD].{tbl} does not exist in the database and will need to be created\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "            return True\n",
    "        elif (stage == True and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG' and table_name = '%s'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG].{tbl} does not exist in the database and will need to be created\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "            return True\n",
    "        elif (stage == False and onb == True):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG_ONB' and table_name = '%s'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG_ONB].{tbl} does not exist in the database and will need to be created\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "            return True\n",
    "\n",
    "    def checkbackups(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_PRD' and table_name = '%s_backup'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_PRD].{tbl}_backup does not exist in the database and will need to be created once parent table has data\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output)\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\nCurrent backup for [GCV_PRD].{tbl} exists.', end = \"\\r\")\n",
    "                logging.info(f'\\nCurrent backup for [GCV_PRD].{tbl} exists.\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        elif (stage == True and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG' and table_name = '%s_backup'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG].{tbl}_backup does not exist in the database and will need to be created once parent table has data\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output)\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\nCurrent backup for [GCV_STG].{tbl}', end = \"\\r\")\n",
    "                logging.info(f'\\nCurrent backup for [GCV_STG].{tbl}\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        elif (stage == False and onb == True):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG_ONB' and table_name = '%s_backup'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG_ONB].{tbl}_backup does not exist in the database and will need to be created once parent table has data\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output)\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\nCurrent backup for [GCV_STG_ONB].{tbl}', end = \"\\r\")\n",
    "                logging.info(f'\\nCurrent backup for [GCV_STG_ONB].{tbl}\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "    \n",
    "    def tablecontent(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_PRD].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            #print(output)\n",
    "            if output[0] == 0:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        elif (stage == True and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            #print(output)\n",
    "            if output[0] == 0:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        elif (stage == False and onb == True):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG_ONB].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            #print(output)\n",
    "            if output[0] == 0:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        \n",
    "    def backup_data(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            if is_identity(con, tbl) == True:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = f\"SET IDENTITY_INSERT [GCV_PRD].[%s_backup] ON; INSERT INTO [GCV_PRD].[%s_backup] ({fields}) SELECT {fields} FROM [GCV_PRD].[%s]; SET IDENTITY_INSERT [GCV_PRD].[%s_backup] OFF;\" % (tbl, tbl, tbl, tbl)\n",
    "                    #print(f'\\n{query}\\n')\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            else:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = \"INSERT INTO [GCV_PRD].[%s_backup] SELECT * FROM [GCV_PRD].[%s]\" % (tbl, tbl)\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            return\n",
    "        elif (stage == True and onb == False):\n",
    "            if is_identity(con, tbl) == True:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = f\"SET IDENTITY_INSERT [GCV_STG].[%s_backup] ON; INSERT INTO [GCV_STG].[%s_backup] ({fields}) SELECT {fields} FROM [GCV_STG].[%s]; SET IDENTITY_INSERT [GCV_STG].[%s_backup] OFF;\" % (tbl, tbl, tbl, tbl)\n",
    "                    #print(f'\\n{query}\\n')\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            else:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = \"INSERT INTO [GCV_STG].[%s_backup] SELECT * FROM [GCV_STG].[%s]\" % (tbl, tbl)\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            return\n",
    "        elif (stage == False and onb == True):\n",
    "            if is_identity(con, tbl) == True:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = f\"SET IDENTITY_INSERT [GCV_STG_ONB].[%s_backup] ON; INSERT INTO [GCV_STG_ONB].[%s_backup] ({fields}) SELECT {fields} FROM [GCV_STG].[%s]; SET IDENTITY_INSERT [GCV_STG].[%s_backup] OFF;\" % (tbl, tbl, tbl, tbl)\n",
    "                    #print(f'\\n{query}\\n')\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            else:\n",
    "                try:\n",
    "                    bstart_time = datetime.now()\n",
    "                    cur = con.cursor()\n",
    "                    query = \"INSERT INTO [GCV_STG_ONB].[%s_backup] SELECT * FROM [GCV_STG_ONB].[%s]\" % (tbl, tbl)\n",
    "                    cur.execute(query)\n",
    "                    con.commit()\n",
    "                    cur.close()\n",
    "                    bend_time = datetime.now()\n",
    "                    print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                    duration = bend_time - bstart_time\n",
    "                    logging.info(f'\\nBackup Duration: {duration}')\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')   \n",
    "                except(Exception, pyodbc.DatabaseError) as e:\n",
    "                    print(\"\")\n",
    "                    logging.info('\\n')\n",
    "                    print(e)\n",
    "                    logging.exception(\"message\")\n",
    "                    cur.close()\n",
    "                    con.rollback()\n",
    "            return\n",
    "\n",
    "    def is_identity(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = f\"SELECT is_identity FROM sys.identity_columns WHERE object_id in ( SELECT id FROM sysobjects WHERE NAME = '%s')\" % (tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                output = cur.fetchone()\n",
    "                if output is None:\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table does not contain a PK_ID\\n')\n",
    "                    return False\n",
    "                else:\n",
    "                    output = output[0]\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table contains a PK_ID, executing special backup procedures.\\n')\n",
    "                    return True\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return \n",
    "        elif (stage == True and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT is_identity FROM sys.identity_columns WHERE object_id in ( SELECT id FROM sysobjects WHERE NAME = '%s')\" % (tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                output = cur.fetchone()\n",
    "                if output is None:\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table does not contain a PK_ID\\n')\n",
    "                    return False\n",
    "                else:\n",
    "                    output = output[0]\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table contains a PK_ID, executing special backup procedures.\\n')\n",
    "                    return True\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return\n",
    "        elif (stage == False and onb == True):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT is_identity FROM sys.identity_columns WHERE object_id in ( SELECT id FROM sysobjects WHERE NAME = '%s')\" % (tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                output = cur.fetchone()\n",
    "                if output is None:\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table does not contain a PK_ID\\n')\n",
    "                    return False\n",
    "                else:\n",
    "                    output = output[0]\n",
    "                    #print(f'\\n{output}\\n')\n",
    "                    print(f'\\nID Check: Table contains a PK_ID, executing special backup procedures.\\n')\n",
    "                    return True\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return\n",
    "            \n",
    "    def get_fields(con, tbl):\n",
    "        global fields\n",
    "        if (stage == False and onb == False):\n",
    "            schema = 'GCV_PRD'\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'\" % (schema, tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                columns = cur.fetchall()\n",
    "                columns = columns[0:]\n",
    "                columns = list(zip(*columns))[0]\n",
    "                fields = \", \".join(map(str, columns))\n",
    "                #print(f'\\n{fields}\\n')\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return fields\n",
    "        elif (stage == True and onb == False):\n",
    "            schema = 'GCV_STG'\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'\" % (schema, tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                columns = cur.fetchall()\n",
    "                columns = columns[0:]\n",
    "                columns = list(zip(*columns))[0]\n",
    "                fields = \", \".join(map(str, columns))\n",
    "                #print(f'\\n{fields}\\n')\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return fields\n",
    "        elif (stage == False and onb == True):\n",
    "            schema = 'GCV_STG_ONB'\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'\" % (schema, tbl)\n",
    "                #print(f'\\n{query}\\n')\n",
    "                cur.execute(query)\n",
    "                columns = cur.fetchall()\n",
    "                columns = columns[0:]\n",
    "                columns = list(zip(*columns))[0]\n",
    "                fields = \", \".join(map(str, columns))\n",
    "                #print(f'\\n{fields}\\n')\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return fields\n",
    "            \n",
    "\n",
    " \n",
    "    def create_backuptable(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT * INTO [GCV_PRD].[%s_backup] FROM [GCV_PRD].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl} backup table has been created.\\n')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                return False\n",
    "            return True\n",
    "        elif (stage == True and onb == False):\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT * INTO [GCV_STG].[%s_backup] FROM [GCV_STG].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                logging.info(f'\\n[GCV_STG].{tbl} backup table has been created.\\n')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                return False\n",
    "            return True\n",
    "        elif (stage == False and onb == True):\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT * INTO [GCV_STG_ONB].[%s_backup] FROM [GCV_STG_ONB].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                logging.info(f'\\n[GCV_STG_ONB].{tbl} backup table has been created.\\n')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "    def backupcheck(con, tbl):\n",
    "        global rowcount\n",
    "        if (stage == False and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_PRD].[%s] UNION ALL SELECT COUNT(*) FROM [GCV_PRD].[%s_backup]\" % (tbl, tbl)\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            rowcount = []\n",
    "            while output is not None:\n",
    "                rowcount.append(output[0])\n",
    "                output = cur.fetchone()\n",
    "            print('\\n# of records in each table: ', rowcount[0], recnum, end = \"\\r\")\n",
    "            logging.info(f'\\n# of records in each table: {rowcount[0]}, {recnum}')\n",
    "            cur.close()\n",
    "            if recnum == rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif recnum < rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif rowcount[0] == 0:\n",
    "                pass\n",
    "            return rowcount\n",
    "        elif (stage == True and onb == False):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG].[%s] UNION ALL SELECT COUNT(*) FROM [GCV_STG].[%s_backup]\" % (tbl, tbl)\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            rowcount = []\n",
    "            while output is not None:\n",
    "                rowcount.append(output[0])\n",
    "                output = cur.fetchone()\n",
    "            print('\\n# of records in each table: ', rowcount[0], recnum, end = \"\\r\")\n",
    "            logging.info(f'\\n# of records in each table: {rowcount[0]}, {recnum}')\n",
    "            cur.close()\n",
    "            if recnum == rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif recnum < rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif rowcount[0] == 0:\n",
    "                pass\n",
    "            return rowcount\n",
    "        elif (stage == False and onb == True):\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG_ONB].[%s] UNION ALL SELECT COUNT(*) FROM [GCV_STG_ONB].[%s_backup]\" % (tbl, tbl)\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            rowcount = []\n",
    "            while output is not None:\n",
    "                rowcount.append(output[0])\n",
    "                output = cur.fetchone()\n",
    "            print('\\n# of records in each table: ', rowcount[0], recnum, end = \"\\r\")\n",
    "            logging.info(f'\\n# of records in each table: {rowcount[0]}, {recnum}')\n",
    "            cur.close()\n",
    "            if recnum == rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif recnum < rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                get_fields(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif rowcount[0] == 0:\n",
    "                pass\n",
    "            return rowcount\n",
    "    \n",
    "    def truncate_table(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_PRD].[%s]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_PRD].{tbl} has been succesfully truncated to import new data.')\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl} has been succesfully truncated to import new data.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        elif (stage == True and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG].[%s]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG].{tbl} has been succesfully truncated to import new data.')\n",
    "                logging.info(f'\\n[GCV_STG].{tbl} has been succesfully truncated to import new data.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        elif (stage == False and onb == True):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG_ONB].[%s]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG_ONB].{tbl} has been succesfully truncated to import new data.')\n",
    "                logging.info(f'\\n[GCV_STG_ONB].{tbl} has been succesfully truncated to import new data.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "            \n",
    "    def truncate_backup(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_PRD].[%s_backup]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_PRD].{tbl}_backup has been succesfully truncated to create the next backup.')\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl}_backup has been succesfully truncated to create the next backup.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        elif (stage == True and onb == False):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG].[%s_backup]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG].{tbl}_backup has been succesfully truncated to create the next backup.')\n",
    "                logging.info(f'\\n[GCV_STG].{tbl}_backup has been succesfully truncated to create the next backup.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        elif (stage == False and onb == True):\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG_ONB].[%s_backup]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG_ONB].{tbl}_backup has been succesfully truncated to create the next backup.')\n",
    "                logging.info(f'\\n[GCV_STG_ONB].{tbl}_backup has been succesfully truncated to create the next backup.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "\n",
    "    def importdata(con, tbl):\n",
    "        if (stage == False and onb == False):\n",
    "            cur = con.cursor()\n",
    "            engine = sa.create_engine(f'mssql+pyodbc://{server}/{database}?driver={dformat}', fast_executemany = True)\n",
    "            #pd.io.sql._is_sqlalchemy_connectable(engine)\n",
    "            df.to_sql(f'{tbl}', engine, index = False, if_exists = 'append', schema = 'GCV_PRD')\n",
    "            cur.close()\n",
    "            con.commit()\n",
    "            return\n",
    "        elif (stage == True and onb == False):\n",
    "            cur = con.cursor()\n",
    "            engine = sa.create_engine(f'mssql+pyodbc://{server}/{database}?driver={dformat}', fast_executemany = True)\n",
    "            #pd.io.sql._is_sqlalchemy_connectable(engine)\n",
    "            df.to_sql(f'{tbl}', engine, index = False, if_exists = 'append', schema = 'GCV_STG')\n",
    "            cur.close()\n",
    "            con.commit()\n",
    "            return\n",
    "        elif (stage == False and onb == True):\n",
    "            cur = con.cursor()\n",
    "            engine = sa.create_engine(f'mssql+pyodbc://{server}/{database}?driver={dformat}', fast_executemany = True)\n",
    "            #pd.io.sql._is_sqlalchemy_connectable(engine)\n",
    "            df.to_sql(f'{tbl}', engine, index = False, if_exists = 'append', schema = 'GCV_STG_ONB')\n",
    "            cur.close()\n",
    "            con.commit()\n",
    "            return\n",
    "            \n",
    "    def getData(CV):\n",
    "        # convert to config file/table\n",
    "        if (stage == False and onb == False):\n",
    "            url = f'https://api.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'config\\apiconnect.json') as f:\n",
    "                headers = json.load(f)\n",
    "        elif (stage == True and onb == False):\n",
    "            url = f'https://api-staging.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'config\\apiconnectb.json') as f:\n",
    "                headers = json.load(f)\n",
    "        elif (stage == False and onb == True):\n",
    "            url = f'https://api-staging.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'config\\apiconnectb.json') as f:\n",
    "                headers = json.load(f)\n",
    "                \n",
    "        retry_strategy = Retry(total = 10, status_forcelist=[429, 413, 503], method_whitelist=[\"HEAD\", \"GET\", \"PUT\", \"DELETE\", \"OPTIONS\", \"TRACE\"])\n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        http = requests.Session()\n",
    "        \n",
    "        http.mount(\"https://\", adapter)\n",
    "        r = http.get(url, headers=headers, timeout = 100)\n",
    "        rjson = r.json()\n",
    "        keylist = ('URI','Categories','RelatedTerms')\n",
    "        \n",
    "        if 'ErrorMessage' in rjson:\n",
    "            if rjson['ErrorMessage'] == 'CV does not exist in CVMaster List':\n",
    "                print('CV Currently Missing from CVMaster List')\n",
    "                return\n",
    "        else:\n",
    "            for key in keylist:\n",
    "                rjson = [{k: v for k, v in d.items() if k != key} for d in rjson]\n",
    "        \n",
    "            global df, recnum\n",
    "            df = pd.DataFrame(rjson)\n",
    "            datelist = ('CreatedDate','ModifiedDate','EffectiveDate', 'HierarchyNodeEffectiveDate', 'HierarchyNodeExpirationDate','RelModifiedDate','ExpiryDate','Effective_Date','Expiration_Date','Created_Datetime','Last_Modified_Datetime')\n",
    "            date_format = \"%Y%m%d%H%M%S\"\n",
    "            \n",
    "            for date in enumerate(datelist): \n",
    "                if date[1] in df:\n",
    "                    \n",
    "                    if df[date[1]].iteritems() != 'None':\n",
    "                        if df[date[1]].str.contains('Z').items():\n",
    "                            df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
    "                    if df[date[1]].iteritems() != 'None':   \n",
    "                        if df[date[1]].str.contains('-').items():\n",
    "                            df[date[1]] = df[date[1]].str.replace(\"\\W+\", \"\")\n",
    "                            \n",
    "                    df[date[1]] = df[date[1]].mask(df[date[1]].str.len() > 14, df[date[1]].str[:-3])\n",
    "                    df[date[1]] = pd.to_datetime(df[date[1]], format=date_format, errors = 'coerce')\n",
    "                    \n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "            recnum = len(df.index)\n",
    "            \n",
    "            #display(df)\n",
    "            #df.to_excel('output1.xlsx')\n",
    "            \n",
    "            if tablecontent(con, f'{CV}') == False:\n",
    "                importdata(con, f'{CV}')\n",
    "                print(f'\\n{CV} has been updated with {recnum} records')\n",
    "                logging.info(f'\\n{CV} has been updated with {recnum} records\\n')\n",
    "            else:\n",
    "                truncate_table(con, f'{CV}')\n",
    "                importdata(con, f'{CV}')\n",
    "                print(f'\\n{CV} has been updated with {recnum} records')\n",
    "                logging.info(f'\\n{CV} has been updated with {recnum} records\\n')\n",
    "                \n",
    "            #colnames = list(df)\n",
    "            \n",
    "            #display(colnames) \n",
    "             #[['Effective_Date','Expiration_Date','Created_Datetime','Last_Modified_Datetime']])\n",
    "            \n",
    "            #--fetch column names for table creation and datatypes\n",
    "            #for colname, dt in itertools.product([df.columns],[df.dtypes]):\n",
    "                #print(dt)\n",
    "            #print('\\n')\n",
    "        \n",
    "            return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(r'config\\CVsStage.csv', 'r') as cv_config:\n",
    "        CVsStage = cv_config.read().split(',')\n",
    "    with open(r'config\\CVsProd.csv', 'r') as cv_config:\n",
    "        CVsProd = cv_config.read().split(',')\n",
    "    with open(r'config\\CVsONB.csv', 'r') as cv_config:\n",
    "        CVsONB = cv_config.read().split(',')\n",
    "\n",
    "    #Used for testing a group of CV's\n",
    "    CVa = ['LEL-PwCLegalEntity-en','NS-PwCNetworkNode-en',\n",
    "           'NS-PwCNetworkNode-en-Territory',\n",
    "           'ORD-CostCenter']\n",
    "    #Used for testing a single CV\n",
    "    CVx = ['ORD-CostCenter']\n",
    "    \n",
    "    CVd = ['ORD-CostCenter']\n",
    "    \n",
    "    st = datetime.now()\n",
    "    open_connection()\n",
    "    connection_test()\n",
    "    close_connection()\n",
    "    \n",
    "    for urls in enumerate(CVsStage):\n",
    "        global stage\n",
    "        stage = True\n",
    "        onb = False\n",
    "        conn = True\n",
    "        open_connection()\n",
    "        #getData(urls[1])\n",
    "        #close_connection()\n",
    "        if checktables(con, urls[1]) == False:\n",
    "            print(\"Moving to next table.\\n\", end = \"\\r\")\n",
    "        else:\n",
    "            if checkbackups(con, urls[1]) == False:\n",
    "                create_backuptable(con, urls[1])\n",
    "                createdbackup = True\n",
    "            else:\n",
    "                createdbackup = False\n",
    "            try:\n",
    "                gstart_time = datetime.now()\n",
    "                getData(urls[1])\n",
    "                if createdbackup == False:\n",
    "                    backupcheck(con, urls[1])\n",
    "                else:\n",
    "                    pass\n",
    "                close_connection()\n",
    "                stage = False\n",
    "                conn = False\n",
    "                gend_time = datetime.now()\n",
    "                print('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                logging.info('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                print(\"\")\n",
    "            except (Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                print(e)\n",
    "                logging.exception('\\n')\n",
    "                logging.exception(\"message\")\n",
    "        for i in range(10, -1, -1):\n",
    "                print(f\"{i} seconds until next table is imported \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "\n",
    "    for urls in enumerate(CVsProd):\n",
    "        open_connection()\n",
    "        stage = False\n",
    "        onb = False\n",
    "        conn = True\n",
    "        #getData(urls[1])\n",
    "        #close_connection()\n",
    "        if checktables(con, urls[1]) == False:\n",
    "            print(\"Moving to next table.\\n\", end = \"\\r\")\n",
    "        else:\n",
    "            if checkbackups(con, urls[1]) == False:\n",
    "                create_backuptable(con, urls[1])\n",
    "                createdbackup = True\n",
    "            else:\n",
    "                createdbackup = False\n",
    "            try:\n",
    "                gstart_time = datetime.now()\n",
    "                getData(urls[1])\n",
    "                if createdbackup == False:\n",
    "                    backupcheck(con, urls[1])\n",
    "                else:\n",
    "                    pass\n",
    "                close_connection()\n",
    "                conn = False\n",
    "                gend_time = datetime.now()\n",
    "                print('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                logging.info('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                print(\"\")\n",
    "            except (Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                print(e)\n",
    "                logging.exception('\\n')\n",
    "                logging.exception(\"message\")\n",
    "        for i in range(10, -1, -1):\n",
    "                print(f\"{i} seconds until next table is imported \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "\n",
    "    for urls in enumerate(CVsONB):\n",
    "        open_connection()\n",
    "        stage = False\n",
    "        onb = True\n",
    "        conn = True\n",
    "        #getData(urls[1])\n",
    "        #close_connection()\n",
    "        if checktables(con, urls[1]) == False:\n",
    "            print(\"Moving to next table.\\n\", end = \"\\r\")\n",
    "        else:\n",
    "            if checkbackups(con, urls[1]) == False:\n",
    "                create_backuptable(con, urls[1])\n",
    "                createdbackup = True\n",
    "            else:\n",
    "                createdbackup = False\n",
    "            try:\n",
    "                gstart_time = datetime.now()\n",
    "                getData(urls[1])\n",
    "                if createdbackup == False:\n",
    "                    backupcheck(con, urls[1])\n",
    "                else:\n",
    "                    pass\n",
    "                close_connection()\n",
    "                onb = False\n",
    "                conn = False\n",
    "                gend_time = datetime.now()\n",
    "                print('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                logging.info('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                print(\"\")\n",
    "            except (Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                print(e)\n",
    "                logging.exception('\\n')\n",
    "                logging.exception(\"message\")\n",
    "        for i in range(10, -1, -1):\n",
    "                print(f\"{i} seconds until next table is imported \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "            \n",
    "    et = datetime.now()\n",
    "    print('Total Execution Duration: {}'.format(et - st),'\\n-Import Completed-')\n",
    "    tt = et - st\n",
    "    logging.info(f'\\nTotal Execution Duration: {tt}\\n')\n",
    "    logging.info('\\n-Import Completed-\\n')\n",
    "    \n",
    "#exceptions \n",
    "except (Exception, pyodbc.DatabaseError) as error:\n",
    "        print(error)\n",
    "        logging.exception(\"message\")\n",
    "        pass\n",
    "    \n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(\"Http Error:\",  errh)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(\"Error Connecting:\", errc)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(\"Timeout Error:\", errt)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.RequestException as erru:\n",
    "    print(\"Unidentified Request Exception:\", erru)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "        \n",
    "finally:\n",
    "    logging.info(f'\\nLOG END: {datetime.now()}\\n')\n",
    "    if conn == True:\n",
    "        close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
