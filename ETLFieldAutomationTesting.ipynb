{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established to:  Microsoft SQL Server 2016 (SP2) (KB4052908) - 13.0.5026.0 (X64) \n",
      "\tMar 18 2018 09:11:49 \n",
      "\tCopyright (c) Microsoft Corporation\n",
      "\tEnterprise Edition (64-bit) on Windows Server 2016 Datacenter 10.0 <X64> (Build 14393: ) (Hypervisor)\n",
      "\n",
      "[('CommonConfig', 'GCV_STG', 'Onboarding_ORD-CostCentreLegalEntity-en-GlobalHierarchy_Denormalised', 'BASE TABLE')]\n",
      "Current backup for [GCV_STG].Onboarding_ORD-CostCentreLegalEntity-en-GlobalHierarchy_Denormalised\n",
      "6581 Existing records\n",
      "[GCV_STG].Onboarding_ORD-CostCentreLegalEntity-en-GlobalHierarchy_Denormalised has been succesfully truncated to import new data.\n",
      "\n",
      "Onboarding_ORD-CostCentreLegalEntity-en-GlobalHierarchy_Denormalised has been updated with 6581 records\n",
      "\n",
      "# of records in each table:  6581 6581\n",
      "[GCV_STG].Onboarding_ORD-CostCentreLegalEntity-en-GlobalHierarchy_Denormalised_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.301510\n",
      "\n",
      "\n",
      "Duration: 0:00:13.024552\n",
      "\n",
      "[('CommonConfig', 'GCV_STG', 'ORD-CostCenter', 'BASE TABLE')]\n",
      "Current backup for [GCV_STG].ORD-CostCenter\n",
      "0 Existing records\n",
      "ORD-CostCenter has been updated with 44030 records\n",
      "\n",
      "# of records in each table:  44030 44030\n",
      "[GCV_STG].ORD-CostCenter_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.436232\n",
      "\n",
      "\n",
      "Duration: 0:01:07.437400\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-JobLevel-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-JobLevel-en exists.\n",
      "23 Existing records\n",
      "[GCV_PRD].NS-JobLevel-en has been succesfully truncated to import new data.\n",
      "\n",
      "NS-JobLevel-en has been updated with 23 records\n",
      "\n",
      "# of records in each table:  23 23\n",
      "[GCV_PRD].NS-JobLevel-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.245694\n",
      "\n",
      "\n",
      "Duration: 0:00:06.123548\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'LEL-PwCLegalEntity-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].LEL-PwCLegalEntity-en exists.\n",
      "1510 Existing records\n",
      "[GCV_PRD].LEL-PwCLegalEntity-en has been succesfully truncated to import new data.\n",
      "\n",
      "LEL-PwCLegalEntity-en has been updated with 1510 records\n",
      "\n",
      "# of records in each table:  1510 1510\n",
      "[GCV_PRD].LEL-PwCLegalEntity-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "('22001', '[22001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]String or binary data would be truncated. (8152) (SQLExecDirectW); [22001] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]The statement has been terminated. (3621)')\n",
      "\n",
      "Duration: 0:01:00.761083\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-ManagementLevel-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-ManagementLevel-en exists.\n",
      "11 Existing records\n",
      "[GCV_PRD].NS-ManagementLevel-en has been succesfully truncated to import new data.\n",
      "\n",
      "NS-ManagementLevel-en has been updated with 11 records\n",
      "\n",
      "# of records in each table:  11 11\n",
      "[GCV_PRD].NS-ManagementLevel-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.253029\n",
      "\n",
      "\n",
      "Duration: 0:00:05.664539\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-PwCNetworkNode-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-PwCNetworkNode-en exists.\n",
      "0 Existing records\n",
      "NS-PwCNetworkNode-en has been updated with 458 records\n",
      "\n",
      "# of records in each table:  458 458\n",
      "[GCV_PRD].NS-PwCNetworkNode-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.258194\n",
      "\n",
      "\n",
      "Duration: 0:00:25.855353\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-PwCNetworkNode-en-Territory', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-PwCNetworkNode-en-Territory exists.\n",
      "0 Existing records\n",
      "NS-PwCNetworkNode-en-Territory has been updated with 113 records\n",
      "\n",
      "# of records in each table:  113 113\n",
      "[GCV_PRD].NS-PwCNetworkNode-en-Territory_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.255906\n",
      "\n",
      "\n",
      "Duration: 0:00:09.521184\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-ResourceRole-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-ResourceRole-en exists.\n",
      "0 Existing records\n",
      "NS-ResourceRole-en has been updated with 14 records\n",
      "\n",
      "# of records in each table:  14 14\n",
      "[GCV_PRD].NS-ResourceRole-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.249224\n",
      "\n",
      "\n",
      "Duration: 0:00:05.815318\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-ResourceType-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-ResourceType-en exists.\n",
      "0 Existing records\n",
      "NS-ResourceType-en has been updated with 2 records\n",
      "\n",
      "# of records in each table:  2 2\n",
      "[GCV_PRD].NS-ResourceType-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.258571\n",
      "\n",
      "\n",
      "Duration: 0:00:05.939176\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-WorkerType-en', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-WorkerType-en exists.\n",
      "2 Existing records\n",
      "[GCV_PRD].NS-WorkerType-en has been succesfully truncated to import new data.\n",
      "\n",
      "NS-WorkerType-en has been updated with 2 records\n",
      "\n",
      "# of records in each table:  2 2\n",
      "[GCV_PRD].NS-WorkerType-en_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.261842\n",
      "\n",
      "\n",
      "Duration: 0:00:05.316683\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'NS-WorkerType-en_NS-ContractType-en_Normalised', 'BASE TABLE')]\n",
      "Current backup for [GCV_PRD].NS-WorkerType-en_NS-ContractType-en_Normalised exists.\n",
      "12 Existing records\n",
      "[GCV_PRD].NS-WorkerType-en_NS-ContractType-en_Normalised has been succesfully truncated to import new data.\n",
      "\n",
      "NS-WorkerType-en_NS-ContractType-en_Normalised has been updated with 12 records\n",
      "\n",
      "# of records in each table:  12 12\n",
      "[GCV_PRD].NS-WorkerType-en_NS-ContractType-en_Normalised_backup has been succesfully truncated to create the next backup.\n",
      "\n",
      "Backup Duration: 0:00:00.241949\n",
      "\n",
      "\n",
      "Duration: 0:00:06.358913\n",
      "\n",
      "[('CommonConfig', 'GCV_PRD', 'ORD-CostCenter', 'BASE TABLE')]\n",
      "[GCV_PRD].ORD-CostCenter_backup does not exist in the database and will need to be created once parent table has data\n",
      "\n",
      "\n",
      "0 Existing records\n",
      "(pyodbc.ProgrammingError) ('42S22', \"[42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Invalid column name 'Record_Status'. (207) (SQLExecute); [42S22] [Microsoft][ODBC Driver 17 for SQL Server][SQL Server]Statement(s) could not be prepared. (8180)\")\n",
      "[SQL: INSERT INTO [GCV_PRD].[ORD-CostCenter] ([Local_Cost_Center_Code], [Cost_Center_Name], [Cost_Center_SK], [Universal_Cost_Center_Code], [Cost_Center_NK], [Cost_Type_Descriptor], [Cost_Type_UID], [PwC_Network_Descriptor], [PwC_Network_UID], [OS_Global_Sub_LoS_Descriptor], [Global_LoS_UID], [OS_Function_Descriptor], [Function_UID], [EmployingFlag], [Cost_Center_Status], [Record_Status], [Effective_Date], [Expiration_Date], [Comments], [Created_Datetime], [Last_Modified_Datetime]) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)]\n",
      "[parameters: (('EVO397', 'Advisory - Consulting ex. Forensics (Reg) (EVO397)', '28805', 'QOEVO397', '12716264EVO397', 'Regional', '12711587', 'CEE BV', '12716264', 'Consulting - FL', '12721147', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2021, 9, 3, 0, 12, 31)), ('EVO809', 'Network Management (EVO809)', '28806', 'QOEVO809', '12716264EVO809', 'Regional', '12711587', 'CEE BV', '12716264', 'Firmwide Management - ZA', '12721189', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2021, 9, 3, 0, 12, 31)), ('EVO815', 'HC Performance - region (EVO815)', '28807', 'QOEVO815', '12716264EVO815', 'Regional', '12711587', 'CEE BV', '12716264', 'Human Capital - ZH', '12721187', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2020, 10, 12, 7, 42, 40)), ('EVO820', 'IT (EVO820)', '28808', 'QOEVO820', '12716264EVO820', 'Regional', '12711587', 'CEE BV', '12716264', 'Central IT - ZT', '12721182', 'Other - Z', '12701038', '1', 'active', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2020, 10, 12, 7, 42, 40)), ('EVO825', 'IT allocations (EVO825)', '28809', 'QOEVO825', '12716264EVO825', 'Regional', '12711587', 'CEE BV', '12716264', 'Central IT - ZT', '12721182', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2021, 9, 3, 0, 12, 31)), ('EVO865', 'Risk Management (EVO865)', '28810', 'QOEVO865', '12716264EVO865', 'Regional', '12711587', 'CEE BV', '12716264', 'Risk Management - ZR', '12721192', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2020, 10, 12, 7, 42, 40)), ('EVO878', 'BOS - Google (EVO878)', '28811', 'QOEVO878', '12716264EVO878', 'Regional', '12711587', 'CEE BV', '12716264', 'Central IT - ZT', '12721182', 'Other - Z', '12701038', '1', 'inactive', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2021, 9, 3, 0, 12, 31)), ('EVO905', 'BOS - Ignite (EVO905)', '28812', 'QOEVO905', '12716264EVO905', 'Regional', '12711587', 'CEE BV', '12716264', 'Human Capital - ZH', '12721187', 'Other - Z', '12701038', '1', 'active', 'active', None, None, None, datetime.datetime(2019, 5, 20, 17, 20, 37), datetime.datetime(2020, 10, 12, 7, 42, 40))  ... displaying 10 of 64972 total bound parameter sets ...  ('AA02', 'C&LD Registration', '8024', 'UKAA02', '1974711AA02', 'Territory', '12711586', 'PwC United Kingdom', '1974711', 'Other National - ZZ', '12721194', 'Positioning - P', '12701029', '1', 'active', 'active', None, None, None, datetime.datetime(2018, 10, 11, 10, 51, 22), datetime.datetime(2020, 1, 22, 19, 40, 20)), ('AA03', 'Barbinder Freehold', '8025', 'UKAA03', '1974711AA03', 'Territory', '12711586', 'PwC United Kingdom', '1974711', 'Other National - ZZ', '12721194', 'Positioning - P', '12701029', '1', 'active', 'active', None, None, None, datetime.datetime(2018, 10, 11, 10, 51, 22), datetime.datetime(2020, 1, 22, 19, 40, 20)))]\n",
      "(Background on this error at: http://sqlalche.me/e/13/f405)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Execution Duration: 0:06:25.951250 \n",
      "-Import Completed-\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import itertools\n",
    "from requests.exceptions import ConnectionError, HTTPError, Timeout, TooManyRedirects\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, event\n",
    "import json\n",
    "import time as ti\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n",
    "\n",
    "con = None\n",
    "try:\n",
    "    \n",
    "    logging.basicConfig(filename = 'CommonConfigLog.log',\n",
    "                        filemode='a',\n",
    "                        format='%(asctime)s,%(msecs)d %(name)s %(levelname)s %(message)s',\n",
    "                        datefmt='%H:%M:%S',\n",
    "                        level=logging.DEBUG)\n",
    "    logging.info(f'\\nLOG START: {datetime.now()}\\n')\n",
    "    \n",
    "    def open_connection():\n",
    "        global server, database, driver, connection, con\n",
    "        with open(r'C:\\Users\\gmoye001\\configtest\\config.json', 'r') as fh:\n",
    "            config = json.load(fh)\n",
    "        server = config['server']\n",
    "        database = config['database']\n",
    "        driver = config['driver']\n",
    "        connection = f'DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection=yes'\n",
    "        con = pyodbc.connect(connection)\n",
    "        return con\n",
    "    \n",
    "    def connection_test():\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT @@version\")\n",
    "        row = cur.fetchone()\n",
    "        print(\"Connection established to: \",row[0])\n",
    "        cur.close()\n",
    "        con.commit()\n",
    "        logging.info(f\"\\nConnection established to: {row[0]}\\n\")\n",
    "        return \n",
    "        \n",
    "    def close_connection():\n",
    "        con.close()\n",
    "        return\n",
    "    \n",
    "    def checktables(con, tbl):\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_PRD' and table_name = '%s'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_PRD].{tbl} does not exist in the database and will need to be created\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "            return True\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG' and table_name = '%s'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG].{tbl} does not exist in the database and will need to be created\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(output, end = \"\\r\")\n",
    "                cur.close()\n",
    "            return True\n",
    "\n",
    "    \n",
    "    def checkbackups(con, tbl):\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_PRD' and table_name = '%s_backup'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_PRD].{tbl}_backup does not exist in the database and will need to be created once parent table has data\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output)\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\nCurrent backup for [GCV_PRD].{tbl} exists.', end = \"\\r\")\n",
    "                logging.info(f'\\nCurrent backup for [GCV_PRD].{tbl} exists.\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM information_schema.Tables WHERE table_schema = 'GCV_STG' and table_name = '%s_backup'\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchall()\n",
    "            if output == []:\n",
    "                output = f'\\n[GCV_STG].{tbl}_backup does not exist in the database and will need to be created once parent table has data\\n'\n",
    "                print(output, end = \"\\r\")\n",
    "                logging.info(output)\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\nCurrent backup for [GCV_STG].{tbl}', end = \"\\r\")\n",
    "                logging.info(f'\\nCurrent backup for [GCV_STG].{tbl}\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "    \n",
    "    def tablecontent(con, tbl):\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_PRD].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            #print(output)\n",
    "            if output[0] == 0:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            #print(output)\n",
    "            if output[0] == 0:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return False\n",
    "            else:\n",
    "                print(f'\\n{output[0]} Existing records', end = \"\\r\")\n",
    "                logging.info(f'\\n{output[0]} Existing records\\n')\n",
    "                cur.close()\n",
    "                return True\n",
    "        \n",
    "    def backup_data(con, tbl):\n",
    "        if stage == False:\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"INSERT INTO [GCV_PRD].[%s_backup] SELECT * FROM [GCV_PRD].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                duration = bend_time - bstart_time\n",
    "                logging.info(f'\\nBackup Duration: {duration}')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return\n",
    "        else:\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"INSERT INTO [GCV_STG].[%s_backup] SELECT * FROM [GCV_STG].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                print('\\nBackup Duration: {}'.format(bend_time - bstart_time))\n",
    "                duration = bend_time - bstart_time\n",
    "                logging.info(f'\\nBackup Duration: {duration}')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "            return\n",
    "        \n",
    "    def create_backuptable(con, tbl):\n",
    "        if stage == False:\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT * INTO [GCV_PRD].[%s_backup] FROM [GCV_PRD].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl} backup table has been created.\\n')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                return False\n",
    "            return True\n",
    "        else:\n",
    "            try:\n",
    "                bstart_time = datetime.now()\n",
    "                cur = con.cursor()\n",
    "                query = \"SELECT * INTO [GCV_STG].[%s_backup] FROM [GCV_STG].[%s]\" % (tbl, tbl)\n",
    "                cur.execute(query)\n",
    "                con.commit()\n",
    "                cur.close()\n",
    "                bend_time = datetime.now()\n",
    "                logging.info(f'\\n[GCV_STG].{tbl} backup table has been created.\\n')\n",
    "                print(\"\")\n",
    "                logging.info('\\n')   \n",
    "            except(Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                logging.info('\\n')\n",
    "                print(e)\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                return False\n",
    "            return True\n",
    "        \n",
    "    def backupcheck(con, tbl):\n",
    "        global rowcount\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_PRD].[%s] UNION ALL SELECT COUNT(*) FROM [GCV_PRD].[%s_backup]\" % (tbl, tbl)\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            rowcount = []\n",
    "            while output is not None:\n",
    "                rowcount.append(output[0])\n",
    "                output = cur.fetchone()\n",
    "            print('\\n# of records in each table: ', rowcount[0], recnum, end = \"\\r\")\n",
    "            logging.info(f'\\n# of records in each table: {rowcount[0]}, {recnum}')\n",
    "            cur.close()\n",
    "            if recnum == rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif recnum < rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif rowcount[0] == 0:\n",
    "                pass\n",
    "            return rowcount\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT COUNT(*) FROM [GCV_STG].[%s] UNION ALL SELECT COUNT(*) FROM [GCV_STG].[%s_backup]\" % (tbl, tbl)\n",
    "            cur.execute(query)\n",
    "            output = cur.fetchone()\n",
    "            rowcount = []\n",
    "            while output is not None:\n",
    "                rowcount.append(output[0])\n",
    "                output = cur.fetchone()\n",
    "            print('\\n# of records in each table: ', rowcount[0], recnum, end = \"\\r\")\n",
    "            logging.info(f'\\n# of records in each table: {rowcount[0]}, {recnum}')\n",
    "            cur.close()\n",
    "            if recnum == rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif recnum < rowcount[0]:\n",
    "                truncate_backup(con, tbl)\n",
    "                backup_data(con, tbl)\n",
    "            elif rowcount[0] == 0:\n",
    "                pass\n",
    "            return rowcount\n",
    "    \n",
    "    def truncate_table(con, tbl):\n",
    "        if stage == False:\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_PRD].[%s]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_PRD].{tbl} has been succesfully truncated to import new data.')\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl} has been succesfully truncated to import new data.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        else:\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG].[%s]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG].{tbl} has been succesfully truncated to import new data.')\n",
    "                logging.info(f'\\n[GCV_STG].{tbl} has been succesfully truncated to import new data.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "            \n",
    "    def truncate_backup(con, tbl):\n",
    "        if stage == False:\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_PRD].[%s_backup]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_PRD].{tbl}_backup has been succesfully truncated to create the next backup.')\n",
    "                logging.info(f'\\n[GCV_PRD].{tbl}_backup has been succesfully truncated to create the next backup.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "        else:\n",
    "            try:\n",
    "                cur = con.cursor()\n",
    "                query = \"TRUNCATE TABLE [GCV_STG].[%s_backup]\" % tbl\n",
    "                cur.execute(query)\n",
    "                print(f'\\n[GCV_STG].{tbl}_backup has been succesfully truncated to create the next backup.')\n",
    "                logging.info(f'\\n[GCV_STG].{tbl}_backup has been succesfully truncated to create the next backup.\\n')\n",
    "                cur.close()\n",
    "                con.commit()\n",
    "            except Exception as err:\n",
    "                logging.exception(\"message\")\n",
    "                cur.close()\n",
    "                con.rollback()\n",
    "                #raise err\n",
    "\n",
    "    def importdata(con, tbl):\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            engine = sa.create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}?Trusted_Connection=yes\", fast_executemany = True)\n",
    "            pd.io.sql._is_sqlalchemy_connectable(engine)\n",
    "            df.to_sql(f'{tbl}', engine, index = False, if_exists = 'append', schema = 'GCV_PRD')\n",
    "            return\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            engine = sa.create_engine(f\"mssql+pyodbc://{server}/{database}?driver={driver}?Trusted_Connection=yes\", fast_executemany = True)\n",
    "            pd.io.sql._is_sqlalchemy_connectable(engine)\n",
    "            df.to_sql(f'{tbl}', engine, index = False, if_exists = 'append', schema = 'GCV_STG')\n",
    "            \n",
    "    def getData(CV):\n",
    "        # convert to config file/table\n",
    "        if stage == False:\n",
    "            url = f'https://api.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'C:\\Users\\gmoye001\\configtest\\apiconnect.json') as f:\n",
    "                headers = json.load(f)\n",
    "        else:\n",
    "            url = f'https://api-staging.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'C:\\Users\\gmoye001\\configtest\\apiconnectb.json') as f:\n",
    "                headers = json.load(f)\n",
    "    \n",
    "        \n",
    "        r = requests.get(url, headers=headers)\n",
    "        rjson = r.json()\n",
    "        keylist = ('URI','Categories','RelatedTerms')\n",
    "        \n",
    "        for key in keylist:\n",
    "            rjson = [{k: v for k, v in d.items() if k != key} for d in rjson]\n",
    "        \n",
    "        global df\n",
    "        df = pd.DataFrame(rjson)\n",
    "        datelist = ('CreatedDate','ModifiedDate','EffectiveDate', 'RelModifiedDate','ExpiryDate','Effective_Date','Expiration_Date','Created_Datetime','Last_Modified_Datetime')\n",
    "        date_format = \"%Y%m%d%H%M%S\"\n",
    "        \n",
    "        for date in enumerate(datelist):\n",
    "            if date[1] in df:\n",
    "                df[date[1]] = df[date[1]].str.replace(\"\\.[0-9]*Z\", \"\").str.replace(\"Z\", \"\")\n",
    "                if date[1] != 'None':\n",
    "                    df[date[1]] = pd.to_datetime(df[date[1]], format=date_format, errors = 'coerce')\n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "        global recnum\n",
    "        recnum = len(df.index)\n",
    "        \n",
    "        #display(df)\n",
    "        #df.to_excel('output1.xlsx')\n",
    "        \n",
    "        if tablecontent(con, f'{CV}') == False:\n",
    "            importdata(con, f'{CV}')\n",
    "            print(f'\\n{CV} has been updated with {recnum} records')\n",
    "            logging.info(f'\\n{CV} has been updated with {recnum} records\\n')\n",
    "        else:\n",
    "            truncate_table(con, f'{CV}')\n",
    "            importdata(con, f'{CV}')\n",
    "            print(f'\\n{CV} has been updated with {recnum} records')\n",
    "            logging.info(f'\\n{CV} has been updated with {recnum} records\\n')\n",
    "            \n",
    "        #colnames = list(df)\n",
    "        \n",
    "        #display(colnames) \n",
    "         #[['Effective_Date','Expiration_Date','Created_Datetime','Last_Modified_Datetime']])\n",
    "        \n",
    "        #--fetch column names for table creation and datatypes\n",
    "        #for colname, dt in itertools.product([df.columns],[df.dtypes]):\n",
    "            #print(dt)\n",
    "        #print('\\n')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    \n",
    "    with open(r'C:\\Users\\gmoye001\\configtest\\CVsStage.csv', 'r') as cv_config:\n",
    "        CVsStage = cv_config.read().split(',')\n",
    "    with open(r'C:\\Users\\gmoye001\\configtest\\CVsProd.csv', 'r') as cv_config:\n",
    "        CVsProd = cv_config.read().split(',')\n",
    "\n",
    "    #Used for testing a group of CV's\n",
    "    CV = ['LEL-PwCLegalEntity-en','NS-PwCNetworkNode-en',\n",
    "           'NS-PwCNetworkNode-en-Territory',\n",
    "           'ORD-CostCenter']\n",
    "    #Used for testing a single CV\n",
    "    CVx = ['ORD-CostCenter']\n",
    "    \n",
    "    CVd = ['ORD-CostCenter']\n",
    "    \n",
    "    st = datetime.now()\n",
    "    open_connection()\n",
    "    connection_test()\n",
    "    close_connection()\n",
    "    \n",
    "    for urls in enumerate(CVsStage):\n",
    "        global stage, conn\n",
    "        stage = True\n",
    "        conn = True\n",
    "        open_connection()\n",
    "        #getData(urls[1])\n",
    "        #close_connection()\n",
    "        if checktables(con, urls[1]) == False:\n",
    "            print(\"Moving to next table.\\n\", end = \"\\r\")\n",
    "        else:\n",
    "            if checkbackups(con, urls[1]) == False:\n",
    "                create_backuptable(con, urls[1])\n",
    "                createdbackup = True\n",
    "            else:\n",
    "                createdbackup = False\n",
    "            try:\n",
    "                gstart_time = datetime.now()\n",
    "                getData(urls[1])\n",
    "                if createdbackup == False:\n",
    "                    backupcheck(con, urls[1])\n",
    "                else:\n",
    "                    pass\n",
    "                close_connection()\n",
    "                stage = False\n",
    "                conn = False\n",
    "                gend_time = datetime.now()\n",
    "                print('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                logging.info('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                print(\"\")\n",
    "            except (Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                print(e)\n",
    "                logging.exception('\\n')\n",
    "                logging.exception(\"message\")\n",
    "        for i in range(10, -1, -1):\n",
    "                print(f\"{i} seconds until next table is imported \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "\n",
    "    for urls in enumerate(CVsProd):\n",
    "        open_connection()\n",
    "        stage = False\n",
    "        conn = True\n",
    "        #getData(urls[1])\n",
    "        #close_connection()\n",
    "        if checktables(con, urls[1]) == False:\n",
    "            print(\"Moving to next table.\\n\", end = \"\\r\")\n",
    "        else:\n",
    "            if checkbackups(con, urls[1]) == False:\n",
    "                create_backuptable(con, urls[1])\n",
    "                createdbackup = True\n",
    "            else:\n",
    "                createdbackup = False\n",
    "            try:\n",
    "                gstart_time = datetime.now()\n",
    "                getData(urls[1])\n",
    "                if createdbackup == False:\n",
    "                    backupcheck(con, urls[1])\n",
    "                else:\n",
    "                    pass\n",
    "                close_connection()\n",
    "                conn = False\n",
    "                gend_time = datetime.now()\n",
    "                print('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                logging.info('\\nDuration: {}'.format(gend_time - gstart_time))\n",
    "                print(\"\")\n",
    "            except (Exception, pyodbc.DatabaseError) as e:\n",
    "                print(\"\")\n",
    "                print(e)\n",
    "                logging.exception('\\n')\n",
    "                logging.exception(\"message\")\n",
    "        for i in range(10, -1, -1):\n",
    "                print(f\"{i} seconds until next table is imported \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "            \n",
    "    et = datetime.now()\n",
    "    print('Total Execution Duration: {}'.format(et - st),'\\n-Import Completed-')\n",
    "    tt = et - st\n",
    "    logging.info(f'\\nTotal Execution Duration: {tt}\\n')\n",
    "    logging.info('\\n-Import Completed-\\n')\n",
    "    \n",
    "#exceptions \n",
    "except (Exception, pyodbc.DatabaseError) as error:\n",
    "        print(error)\n",
    "        logging.exception(\"message\")\n",
    "        pass\n",
    "    \n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(\"Http Error:\",  errh)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(\"Error Connecting:\", errc)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(\"Timeout Error:\", errt)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "    \n",
    "except requests.exceptions.RequestException as erru:\n",
    "    print(\"Unidentified Request Exception:\", erru)\n",
    "    conn = False\n",
    "    logging.exception(\"message\")\n",
    "        \n",
    "finally:\n",
    "    logging.info(f'\\nLOG END: {datetime.now()}\\n')\n",
    "    if conn == True:\n",
    "        close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection established to:  Microsoft SQL Server 2016 (SP2) (KB4052908) - 13.0.5026.0 (X64) \n",
      "\tMar 18 2018 09:11:49 \n",
      "\tCopyright (c) Microsoft Corporation\n",
      "\tEnterprise Edition (64-bit) on Windows Server 2016 Datacenter 10.0 <X64> (Build 14393: ) (Hypervisor)\n",
      "\n",
      "['CostCenterPwCNetworkNodeId', 'CostCenterPwCNetworkDescriptor', 'CostCenterId', 'LocalCostCenterCode', 'CostCenterName', 'UniversalCostCenterCode', 'LegalEntityPartyId', 'LegalEntityName', 'ModifiedDate'] \n",
      "\n",
      "['CostCenterPwCNetworkNodeId', 'CostCenterPwCNetworkDescriptor', 'CostCenterId', 'LocalCostCenterCode', 'CostCenterName', 'UniversalCostCenterCode', 'LegalEntityPartyId', 'LegalEntityName', 'ModifiedDate'] \n",
      "\n",
      "['Local_Cost_Center_Code', 'Cost_Center_Name', 'Cost_Center_SK', 'Universal_Cost_Center_Code', 'Cost_Center_NK', 'Cost_Type_Descriptor', 'Cost_Type_UID', 'PwC_Network_Descriptor', 'PwC_Network_UID', 'OS_Global_Sub_LoS_Descriptor', 'Global_LoS_UID', 'OS_Function_Descriptor', 'Function_UID', 'Employing_Flag', 'Cost_Center_Status', 'Active_Status', 'Effective_Date', 'Expiration_Date', 'Comments', 'Ownership_Folder', 'Created_Datetime', 'Last_Modified_Datetime', 'Batch_Number'] \n",
      "\n",
      "['Local_Cost_Center_Code', 'Cost_Center_Name', 'Cost_Center_SK', 'Universal_Cost_Center_Code', 'Cost_Center_NK', 'Cost_Type_Descriptor', 'Cost_Type_UID', 'PwC_Network_Descriptor', 'PwC_Network_UID', 'OS_Global_Sub_LoS_Descriptor', 'Global_LoS_UID', 'OS_Function_Descriptor', 'Function_UID', 'Employing_Flag', 'Cost_Center_Status', 'Active_Status', 'Effective_Date', 'Expiration_Date', 'Comments', 'Ownership_Folder', 'Created_Datetime', 'Last_Modified_Datetime', 'Batch_Number'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'CreatedDate', 'ModifiedDate', 'JobLevelName', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SortOrder'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'ApprovalStatus', 'CreatedDate', 'ModifiedDate', 'JobLevelName', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SortOrder'] \n",
      "\n",
      "['PartyReferenceId', 'PartyId', 'Name', 'ResponsibleNetworkNodeId', 'ResponsibleNetworkNode', 'OperatingNetworkNodeId', 'OperatingNetworkNode', 'EmployingNetworkNodeId', 'EmployingNetworkNode', 'DUNS', 'LELNumber', 'GFSId', 'RegisteredAddressLine1', 'RegisteredAddressLine2', 'RegisteredAddressLine3', 'RegisteredCity', 'RegisteredCountrySubdivisionId', 'RegisteredCountrySubdivisionCode', 'RegisteredCountrySubdivisionName', 'RegisteredCountryId', 'RegisteredCountryCode', 'RegisteredCountryName', 'RegisteredPostalCode', 'OperatingAddressLine1', 'OperatingAddressLine2', 'OperatingAddressLine3', 'OperatingCity', 'OperatingCountrySubdivisionId', 'OperatingCountrySubdivisionCode', 'OperatingCountrySubdivisionName', 'OperatingCountryId', 'OperatingCountryCode', 'OperatingCountryName', 'OperatingPostalCode', 'BillingAddressLine1', 'BillingAddressLine2', 'BillingAddressLine3', 'BillingCity', 'BillingCountrySubdivisionId', 'BillingCountrySubdivisionCode', 'BillingCountrySubdivisionName', 'BillingCountryId', 'BillingCountryCode', 'BillingCountryName', 'BillingPostalCode', 'EmployingLE', 'CreatedDate', 'PartyRoleId', 'PartyRole', 'ModifiedDate', 'LicenseTypeId', 'LicenseType', 'OrganisationTypeId', 'OrganisationType'] \n",
      "\n",
      "['PartyReferenceId', 'PartyId', 'Name', 'ResponsibleNetworkNodeId', 'ResponsibleNetworkNode', 'OperatingNetworkNodeId', 'OperatingNetworkNode', 'EmployingNetworkNodeId', 'EmployingNetworkNode', 'DUNS', 'LELNumber', 'GFSId', 'RegisteredAddressLine1', 'RegisteredAddressLine2', 'RegisteredAddressLine3', 'RegisteredCity', 'RegisteredCountrySubdivisionId', 'RegisteredCountrySubdivisionCode', 'RegisteredCountrySubdivisionName', 'RegisteredCountryId', 'RegisteredCountryCode', 'RegisteredCountryName', 'RegisteredPostalCode', 'OperatingAddressLine1', 'OperatingAddressLine2', 'OperatingAddressLine3', 'OperatingCity', 'OperatingCountrySubdivisionId', 'OperatingCountrySubdivisionCode', 'OperatingCountrySubdivisionName', 'OperatingCountryId', 'OperatingCountryCode', 'OperatingCountryName', 'OperatingPostalCode', 'BillingAddressLine1', 'BillingAddressLine2', 'BillingAddressLine3', 'BillingCity', 'BillingCountrySubdivisionId', 'BillingCountrySubdivisionCode', 'BillingCountrySubdivisionName', 'BillingCountryId', 'BillingCountryCode', 'BillingCountryName', 'BillingPostalCode', 'EmployingLE', 'CreatedDate', 'PartyRoleId', 'PartyRole', 'ModifiedDate', 'LicenseTypeId', 'LicenseType', 'OrganisationTypeId', 'OrganisationType'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'CreatedDate', 'ModifiedDate', 'ManagementLevelName', 'Abbreviation', 'Definition', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SortOrder', 'GERCode', 'Comments', 'ExpiryDate'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'ApprovalStatus', 'CreatedDate', 'ModifiedDate', 'ManagementLevelName', 'Abbreviation', 'Definition', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SortOrder', 'GERCode', 'Comments', 'ExpiryDate'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'CreatedDate', 'ModifiedDate', 'NodeName', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SourceMDMGERCode', 'SourceMDMSortPrefix', 'PRID', 'PartyID', 'NetworkNodeID', 'LocalAccountingCurrency', 'RegionalReportingCurrency', 'Disambiguator', 'Comments', 'GGCEPrefix', 'Definition', 'ExpiryDate', 'Abbreviation', 'SCMember', 'NLTMember', 'NodeTypeId', 'NodeType'] \n",
      "\n",
      "['Id', 'Name', 'VocabName', 'ActiveStatus', 'ApprovalStatus', 'CreatedDate', 'ModifiedDate', 'NodeName', 'EffectiveDate', 'LastEditedReason', 'Systemofrecord', 'SourceMDMGERCode', 'SourceMDMSortPrefix', 'PRID', 'PartyID', 'NetworkNodeID', 'LocalAccountingCurrency', 'RegionalReportingCurrency', 'Disambiguator', 'Comments', 'GGCEPrefix', 'Definition', 'ExpiryDate', 'Abbreviation', 'SCMember', 'NLTMember', 'NodeTypeId', 'NodeType', 'TBDRateElementId', 'TBDRateElementName', 'TBDRateElementLevel'] \n",
      "\n",
      "Waiting 1 seconds to ensure stability \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-642edd96a97c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Waiting {i} seconds to ensure stability \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[0mti\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Pseudocode for automated field creation/deletion\n",
    "\n",
    "import requests\n",
    "import urllib\n",
    "import itertools\n",
    "from requests.exceptions import ConnectionError, HTTPError, Timeout, TooManyRedirects\n",
    "import traceback\n",
    "import logging\n",
    "import logging.handlers\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyodbc\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, event\n",
    "import json\n",
    "import time as ti\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "global df, df2\n",
    "con = None\n",
    "try:\n",
    "    \n",
    "    def open_connection():\n",
    "        global server, database, driver, connection, con\n",
    "        with open(r'C:\\Users\\gmoye001\\configtest\\config.json', 'r') as fh:\n",
    "            config = json.load(fh)\n",
    "        server = config['server']\n",
    "        database = config['database']\n",
    "        driver = config['driver']\n",
    "        connection = f'DRIVER={driver};SERVER={server};DATABASE={database};Trusted_Connection=yes'\n",
    "        con = pyodbc.connect(connection)\n",
    "        return con\n",
    "    \n",
    "    def connection_test():\n",
    "        cur = con.cursor()\n",
    "        cur.execute(\"SELECT @@version\")\n",
    "        row = cur.fetchone()\n",
    "        print(\"Connection established to: \",row[0])\n",
    "        cur.close()\n",
    "        con.commit()\n",
    "        return \n",
    "        \n",
    "    def close_connection():\n",
    "        con.close()\n",
    "        return\n",
    "    \n",
    "    def fetch_dbtbl_column_names(con, tbl):\n",
    "        if stage == False:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM [GCV_PRD].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            dbcols = [column[0] for column in cur.description]\n",
    "            cur.close()\n",
    "            return dbcols\n",
    "        else:\n",
    "            cur = con.cursor()\n",
    "            query = \"SELECT * FROM [GCV_STG].[%s]\" % tbl\n",
    "            cur.execute(query)\n",
    "            dbcols = [column[0] for column in cur.description]\n",
    "            cur.close()\n",
    "            return dbcols\n",
    "        \n",
    "    #def create_field():\n",
    "    #    global df, df2\n",
    "    #    for field in enumerate(apifields):\n",
    "    #        if field[1] not in dbtblfields:\n",
    "    #            print('creating db table field')\n",
    "    #            \n",
    "    #            if df[f'{field[1]}'].dtype == 'int64':\n",
    "    #                datatype = 'BigInt'\n",
    "    #            if df[f'{field[1]}'].dtype == 'object':\n",
    "    #                print(max(df.loc[:,field[1]].apply(len)))\n",
    "    #                if max(df.loc[:,field[1]].apply(len)) < 255:\n",
    "    #                    datatype = 'nvarchar(510)'\n",
    "    #                elif max(df.loc[:, field[1]].apply(len)) > 255:\n",
    "    #                    datatype = 'varchar(MAX)'\n",
    "    #            if 'Date' in field[1]:\n",
    "    #                datatype = 'datetime'  \n",
    "    #            else:\n",
    "    #                datatype = df[f'{field[1]}'].dtype\n",
    "    #            print('datatype is', df[f'{field[1]}'].dtype, ', Post Analysis datatype is', datatype,', fieldname is', field[1])\n",
    "    #            df2 = pd.concat([df2, df[f'{field[1]}']], axis =1)\n",
    "    #    return print('Field Creation Complete')#\n",
    "    #    \n",
    "    #def delete_field():\n",
    "    #    global df, df2\n",
    "    #    for field in enumerate(dbtblfields):\n",
    "    #        if field[1] not in apifields:\n",
    "    #            print('deleting db table field')\n",
    "    #            print('datatype is', df2[f'{field[1]}'].dtype, ', fieldname is', field[1])\n",
    "    #            del df2[f'{field[1]}']\n",
    "    #    return print('Field Deletion Complete')\n",
    "    #\n",
    "    #print(len(apifields), 'fields in the api table')\n",
    "    #print(len(dbtblfields[:-1]), 'fields in the database table')\n",
    "    #\n",
    "    #if len(apifields) > len(dbtblfields[:-1]):\n",
    "    #    create_field()\n",
    "    #    df2.insert(len(df2)+1, 'importTime', df2.pop('importTime'))\n",
    "    #    display(df, df2)\n",
    "    #if len(apifields) < len(dbtblfields[:-1]):\n",
    "    #    delete_field()\n",
    "    #    display(df, df2)\n",
    "    \n",
    "    with open(r'C:\\Users\\gmoye001\\configtest\\CVsStage.csv', 'r') as cv_config:\n",
    "        CVsStage = cv_config.read().split(',')\n",
    "    with open(r'C:\\Users\\gmoye001\\configtest\\CVsProd.csv', 'r') as cv_config:\n",
    "        CVsProd = cv_config.read().split(',')\n",
    "\n",
    "    def getData(CV):\n",
    "        \n",
    "        if stage == False:\n",
    "            url = f'https://api.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'C:\\Users\\gmoye001\\configtest\\apiconnect.json') as f:\n",
    "                headers = json.load(f)\n",
    "        else:\n",
    "            url = f'https://api-staging.pwcinternal.com:7443/GlobalCVService/GlobalCVService.svc/cv/{CV}'\n",
    "            with open(r'C:\\Users\\gmoye001\\configtest\\apiconnectb.json') as f:\n",
    "                headers = json.load(f)\n",
    "    \n",
    "        \n",
    "        r = requests.get(url, headers=headers)\n",
    "        rjson = r.json()\n",
    "        keylist = ('URI','Categories','RelatedTerms')\n",
    "        \n",
    "        for key in keylist:\n",
    "            rjson = [{k: v for k, v in d.items() if k != key} for d in rjson]\n",
    "        \n",
    "        global df\n",
    "        df = pd.DataFrame(rjson)\n",
    "        \n",
    "        apifields = list(df.columns)\n",
    "        #fetch_dbtbl_column_names(con, f'{CV}')\n",
    "        \n",
    "        print(apifields, '\\n')\n",
    "        dbcols = fetch_dbtbl_column_names(con, f'{CV}')\n",
    "        print(dbcols[:-1], '\\n')\n",
    "        \n",
    "    open_connection()\n",
    "    connection_test()\n",
    "    close_connection()\n",
    "    \n",
    "    for urls in enumerate(CVsStage):\n",
    "        global stage, conn\n",
    "        open_connection()\n",
    "        stage = True\n",
    "        conn = True\n",
    "        getData(urls[1])\n",
    "        close_connection()\n",
    "        conn = False\n",
    "        for i in range(2, -1, -1):\n",
    "                print(f\"Waiting {i} seconds to ensure stability \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "\n",
    "    for urls in enumerate(CVsProd):\n",
    "        open_connection()\n",
    "        stage = False\n",
    "        conn = True\n",
    "        getData(urls[1])\n",
    "        close_connection()\n",
    "        conn = False\n",
    "        for i in range(2, -1, -1):\n",
    "                print(f\"Waiting {i} seconds to ensure stability \", end = \"\\r\")\n",
    "                ti.sleep(1)\n",
    "    \n",
    "\n",
    "\n",
    "#def create_field(con, tbl):\n",
    "#    for fields in enumerate(apifields):\n",
    "#        if stage == False:\n",
    "#            if fields[1] not in dbtblfields\n",
    "#                cur = con.cursor()\n",
    "#                query = f\"ALTER TABLE [GCV_PRD].[%s] ADD {fields[1]} ;\" % tbl\n",
    "#                cur.execute(query)\n",
    "#                output = cur.fetchall()\n",
    "#                cur.close()\n",
    "#                print(output, end = \"\\r\")\n",
    "#                logging.info(output, end = \"\\r\")\n",
    "#\n",
    "#        else:\n",
    "#            if fields[1] not in dbtblfields\n",
    "#                cur = con.cursor()\n",
    "#                query = f\"ALTER TABLE [GCV_STG].[%s] ADD {fields[1]} ;\" % tbl\n",
    "#                cur.execute(query)\n",
    "#                output = cur.fetchall()\n",
    "#                cur.close()\n",
    "#                print(output, end = \"\\r\")\n",
    "#                logging.info(output, end = \"\\r\")\n",
    "#        return\n",
    "#    \n",
    "#def delete_field(con, tbl):\n",
    "#    for fields in enumerate(apifields):\n",
    "#        if stage == False:\n",
    "#            if fields[1] not in apifields\n",
    "#                cur = con.cursor()\n",
    "#                query = f\"ALTER TABLE [GCV_PRD].[%s] DROP COLUMN {fields[1]};\" % tbl\n",
    "#                cur.execute(query)\n",
    "#                output = cur.fetchall()\n",
    "#                cur.close()\n",
    "#                print(output, end = \"\\r\")\n",
    "#                logging.info(output, end = \"\\r\")\n",
    "#        else:\n",
    "#            if fields[1] not in dbtblfields\n",
    "#                cur = con.cursor()\n",
    "#                query = f\"ALTER TABLE [GCV_STG].[%s] DROP COLUMN {fields[1]};\" % tbl\n",
    "#                cur.execute(query)\n",
    "#                output = cur.fetchall()\n",
    "#                cur.close()\n",
    "#                print(output, end = \"\\r\")\n",
    "#                logging.info(output, end = \"\\r\")\n",
    "#        return\n",
    "\n",
    "#exceptions \n",
    "except (Exception, pyodbc.DatabaseError) as error:\n",
    "        print(error)\n",
    "        pass\n",
    "    \n",
    "except requests.exceptions.HTTPError as errh:\n",
    "    print(\"Http Error:\",  errh)\n",
    "    con = False\n",
    "    \n",
    "except requests.exceptions.ConnectionError as errc:\n",
    "    print(\"Error Connecting:\", errc)\n",
    "    con = False\n",
    "    \n",
    "except requests.exceptions.Timeout as errt:\n",
    "    print(\"Timeout Error:\", errt)\n",
    "    con = False\n",
    "    \n",
    "except requests.exceptions.RequestException as erru:\n",
    "    print(\"Unidentified Request Exception:\", erru)\n",
    "    con = False\n",
    "        \n",
    "finally:\n",
    "    if conn == True:\n",
    "        close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
